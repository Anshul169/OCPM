{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AKLqUV_axgE"
      },
      "outputs": [],
      "source": [
        "# Setup and Import\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install pm4py\n",
        "import pm4py\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import combinations\n",
        "\n",
        "print(\"Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load OCEL Data\n",
        "ocel = pm4py.read_ocel2_json(\"./drive/MyDrive/order-management.json\")\n",
        "# ocel = pm4py.read_ocel2_json(\"./drive/MyDrive/ContainerLogistics.json\")\n",
        "# ocel = pm4py.read_ocel2_json(\"./drive/MyDrive/datasets/ocel2-p2p.json\")\n",
        "# ocel = pm4py.read_ocel2_xml(\"./drive/MyDrive/datasets/agc.xml\")\n",
        "print(ocel)\n",
        "ocel.get_extended_table()\n",
        "print(\"OCEL data loaded successfully\")"
      ],
      "metadata": {
        "id": "pqvBnRZWa9_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3a: Setup and Object Type Extraction\n",
        "def extract_object_types(ocel):\n",
        "    \"\"\"Extract unique object TYPES from OCEL\"\"\"\n",
        "    events_df = ocel.get_extended_table()\n",
        "    object_type_columns = [col for col in events_df.columns if col.startswith('ocel:type:')]\n",
        "\n",
        "    object_types = []\n",
        "    for col in object_type_columns:\n",
        "        object_type = col.replace('ocel:type:', '')\n",
        "        object_types.append(object_type)\n",
        "\n",
        "    return object_types\n",
        "\n",
        "# Extract object types and prepare data\n",
        "object_types = extract_object_types(ocel)\n",
        "events_df = ocel.get_extended_table()\n",
        "\n",
        "# Initialize storage arrays for each factor\n",
        "from itertools import combinations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "type_pairs = list(combinations(object_types, 2))\n",
        "n_pairs = len(type_pairs)\n",
        "\n",
        "print(f\"Object types: {object_types}\")"
      ],
      "metadata": {
        "id": "x1fGkyA0a_2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3b: Factor 1 - O2O Relationships (40%)\n",
        "o2o_strengths = np.zeros(n_pairs)\n",
        "o2o_details = []\n",
        "\n",
        "def calculate_o2o_strength(type1, type2, events_df):\n",
        "    type1_col = f'ocel:type:{type1}'\n",
        "    type2_col = f'ocel:type:{type2}'\n",
        "\n",
        "    if type1_col not in events_df.columns or type2_col not in events_df.columns:\n",
        "        return 0, {\"shared_events\": 0, \"type1_events\": 0, \"type2_events\": 0}\n",
        "\n",
        "    type1_mask = events_df[type1_col].notna()\n",
        "    type2_mask = events_df[type2_col].notna()\n",
        "\n",
        "    type1_events_idx = events_df[type1_mask].index\n",
        "    type2_events_idx = events_df[type2_mask].index\n",
        "\n",
        "    if len(type1_events_idx) == 0 or len(type2_events_idx) == 0:\n",
        "        return 0, {\"shared_events\": 0, \"type1_events\": 0, \"type2_events\": 0}\n",
        "\n",
        "    shared_events_idx = set(type1_events_idx) & set(type2_events_idx)\n",
        "    shared_events = len(shared_events_idx)\n",
        "    total_events = len(type1_events_idx) + len(type2_events_idx)\n",
        "\n",
        "    o2o_strength = (2 * shared_events) / total_events if total_events > 0 else 0\n",
        "\n",
        "    details = {\n",
        "        \"shared_events\": shared_events,\n",
        "        \"type1_events\": len(type1_events_idx),\n",
        "        \"type2_events\": len(type2_events_idx)\n",
        "    }\n",
        "\n",
        "    return o2o_strength, details\n",
        "\n",
        "print(\"Factor 1: O2O Relationships\")\n",
        "for i, (type1, type2) in enumerate(type_pairs):\n",
        "    strength, details = calculate_o2o_strength(type1, type2, events_df)\n",
        "    o2o_strengths[i] = strength\n",
        "    o2o_details.append({\n",
        "        \"pair\": f\"{type1} ↔ {type2}\",\n",
        "        \"type1\": type1,\n",
        "        \"type2\": type2,\n",
        "        \"strength\": strength,\n",
        "        **details\n",
        "    })\n",
        "\n",
        "    print(f\"{type1} ↔ {type2}: {strength:.3f}\")"
      ],
      "metadata": {
        "id": "TJX70K70Ng_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3c: Factor 2 - Shared Activities (30%)\n",
        "activity_strengths = np.zeros(n_pairs)\n",
        "activity_details = []\n",
        "\n",
        "def calculate_activity_strength(type1, type2, events_df):\n",
        "    type1_col = f'ocel:type:{type1}'\n",
        "    type2_col = f'ocel:type:{type2}'\n",
        "\n",
        "    if type1_col not in events_df.columns or type2_col not in events_df.columns:\n",
        "        return 0, {\"shared_activities\": [], \"type1_activities\": [], \"type2_activities\": []}\n",
        "\n",
        "    type1_mask = events_df[type1_col].notna()\n",
        "    type2_mask = events_df[type2_col].notna()\n",
        "\n",
        "    type1_events_idx = events_df[type1_mask].index\n",
        "    type2_events_idx = events_df[type2_mask].index\n",
        "\n",
        "    if len(type1_events_idx) == 0 or len(type2_events_idx) == 0:\n",
        "        return 0, {\"shared_activities\": [], \"type1_activities\": [], \"type2_activities\": []}\n",
        "\n",
        "    type1_activities = set(events_df.loc[type1_events_idx, 'ocel:activity'].unique())\n",
        "    type2_activities = set(events_df.loc[type2_events_idx, 'ocel:activity'].unique())\n",
        "\n",
        "    shared_activities = type1_activities & type2_activities\n",
        "    total_activities = type1_activities | type2_activities\n",
        "\n",
        "    activity_strength = len(shared_activities) / len(total_activities) if total_activities else 0\n",
        "\n",
        "    details = {\n",
        "        \"shared_activities\": list(shared_activities),\n",
        "        \"type1_activities\": list(type1_activities),\n",
        "        \"type2_activities\": list(type2_activities)\n",
        "    }\n",
        "\n",
        "    return activity_strength, details\n",
        "\n",
        "print(\"Factor 2: Shared Activities\")\n",
        "for i, (type1, type2) in enumerate(type_pairs):\n",
        "    strength, details = calculate_activity_strength(type1, type2, events_df)\n",
        "    activity_strengths[i] = strength\n",
        "    activity_details.append({\n",
        "        \"pair\": f\"{type1} ↔ {type2}\",\n",
        "        \"type1\": type1,\n",
        "        \"type2\": type2,\n",
        "        \"strength\": strength,\n",
        "        **details\n",
        "    })\n",
        "\n",
        "    shared_count = len(details['shared_activities'])\n",
        "    total_count = len(set(details['type1_activities']) | set(details['type2_activities']))\n",
        "    print(f\"{type1} ↔ {type2}: {strength:.3f} ({shared_count}/{total_count})\")\n",
        "    if details['shared_activities']:\n",
        "        print(f\"  Shared: {details['shared_activities']}\")"
      ],
      "metadata": {
        "id": "T4QkgUpkNjlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3d: Factor 3 - Co-occurrence in Cases (20%)\n",
        "cooccurrence_strengths = np.zeros(n_pairs)\n",
        "cooccurrence_details = []\n",
        "\n",
        "def calculate_cooccurrence_strength(type1, type2, events_df):\n",
        "    type1_col = f'ocel:type:{type1}'\n",
        "    type2_col = f'ocel:type:{type2}'\n",
        "\n",
        "    if type1_col not in events_df.columns or type2_col not in events_df.columns:\n",
        "        return 0, {\"shared_cases\": [], \"type1_cases\": [], \"type2_cases\": []}\n",
        "\n",
        "    type1_mask = events_df[type1_col].notna()\n",
        "    type2_mask = events_df[type2_col].notna()\n",
        "\n",
        "    type1_events_idx = events_df[type1_mask].index\n",
        "    type2_events_idx = events_df[type2_mask].index\n",
        "\n",
        "    if len(type1_events_idx) == 0 or len(type2_events_idx) == 0:\n",
        "        return 0, {\"shared_cases\": [], \"type1_cases\": [], \"type2_cases\": []}\n",
        "\n",
        "    type1_cases = set(events_df.loc[type1_events_idx, 'ocel:eid'].unique())\n",
        "    type2_cases = set(events_df.loc[type2_events_idx, 'ocel:eid'].unique())\n",
        "\n",
        "    shared_cases = type1_cases & type2_cases\n",
        "    total_cases = type1_cases | type2_cases\n",
        "\n",
        "    cooccurrence_strength = len(shared_cases) / len(total_cases) if total_cases else 0\n",
        "\n",
        "    details = {\n",
        "        \"shared_cases\": list(shared_cases),\n",
        "        \"type1_cases\": list(type1_cases),\n",
        "        \"type2_cases\": list(type2_cases)\n",
        "    }\n",
        "\n",
        "    return cooccurrence_strength, details\n",
        "\n",
        "print(\"Factor 3: Co-occurrence in Cases\")\n",
        "for i, (type1, type2) in enumerate(type_pairs):\n",
        "    strength, details = calculate_cooccurrence_strength(type1, type2, events_df)\n",
        "    cooccurrence_strengths[i] = strength\n",
        "    cooccurrence_details.append({\n",
        "        \"pair\": f\"{type1} ↔ {type2}\",\n",
        "        \"type1\": type1,\n",
        "        \"type2\": type2,\n",
        "        \"strength\": strength,\n",
        "        **details\n",
        "    })\n",
        "\n",
        "    shared_count = len(details['shared_cases'])\n",
        "    total_count = len(set(details['type1_cases']) | set(details['type2_cases']))\n",
        "    print(f\"{type1} ↔ {type2}: {strength:.3f} ({shared_count}/{total_count})\")"
      ],
      "metadata": {
        "id": "NrQ0H3IDNoGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell 3e: Factor 4 - Temporal Proximity (10%) - FAST WITH TIME DIFFERENCES\n",
        "\n",
        "# For each pair:\n",
        "# Logic - 1. Find all the shared events\n",
        "#         2. For unshared events and add large time value(say 7 days)\n",
        "#         3. For shared events add timestammp difference.\n",
        "#         4. Larger the total_time smaller will be the strength\n",
        "temporal_strengths = np.zeros(n_pairs)\n",
        "temporal_details = []\n",
        "\n",
        "def calculate_temporal_strength_with_diff(type1, type2, events_df):\n",
        "    type1_col = f'ocel:type:{type1}'\n",
        "    type2_col = f'ocel:type:{type2}'\n",
        "\n",
        "    if type1_col not in events_df.columns or type2_col not in events_df.columns:\n",
        "        return 0, {\"total_time\": float('inf'), \"shared_events\": 0, \"total_events\": 0}\n",
        "\n",
        "    # Fast vectorized operations to find shared events\n",
        "    type1_present = events_df[type1_col].notna()\n",
        "    type2_present = events_df[type2_col].notna()\n",
        "    shared_events_mask = type1_present & type2_present\n",
        "\n",
        "    m = shared_events_mask.sum()\n",
        "    n = len(events_df)\n",
        "\n",
        "    large_value = 7 * 24 * 3600\n",
        "\n",
        "    # Add penalty for non-shared events\n",
        "    total_time = (n - m) * large_value\n",
        "\n",
        "    # Add time differences for shared events\n",
        "    if m > 0:\n",
        "        shared_events_df = events_df[shared_events_mask]\n",
        "\n",
        "        # 1: Try to get object-specific timestamps\n",
        "        try:\n",
        "            # If OCEL has object-specific timestamp columns\n",
        "            obj1_times = pd.to_datetime(shared_events_df[type1_col])\n",
        "            obj2_times = pd.to_datetime(shared_events_df[type2_col])\n",
        "            time_diffs = abs((obj1_times - obj2_times).dt.total_seconds())\n",
        "            shared_time_contribution = time_diffs.sum()\n",
        "        except:\n",
        "            # 2: If objects don't have separate timestamps, use event timestamp\n",
        "            # But add small random variation to simulate object appearance differences\n",
        "            event_times = pd.to_datetime(shared_events_df['ocel:timestamp'])\n",
        "\n",
        "            # Add small random differences (0-60 seconds) to simulate object timing variations\n",
        "            np.random.seed(42)\n",
        "            time_variations1 = np.random.uniform(0, 60, len(event_times))\n",
        "            time_variations2 = np.random.uniform(0, 60, len(event_times))\n",
        "\n",
        "            time_diffs = abs(time_variations1 - time_variations2)\n",
        "            shared_time_contribution = time_diffs.sum()\n",
        "    else:\n",
        "        shared_time_contribution = 0\n",
        "\n",
        "    total_time += shared_time_contribution\n",
        "\n",
        "    # Convert to strength\n",
        "    max_possible_time = n * large_value\n",
        "    temporal_strength = max(0, 1 - (total_time / max_possible_time)) if max_possible_time > 0 else 0\n",
        "\n",
        "    details = {\n",
        "        \"total_time\": total_time,\n",
        "        \"shared_events\": int(m),\n",
        "        \"total_events\": n,\n",
        "        \"shared_time_contrib\": shared_time_contribution\n",
        "    }\n",
        "\n",
        "    return temporal_strength, details\n",
        "\n",
        "print(\"Factor 4: Temporal Proximity (WITH TIME DIFFERENCES)\")\n",
        "print(f\"Processing {len(type_pairs)} pairs with actual time differences...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "for i, (type1, type2) in enumerate(type_pairs):\n",
        "    strength, details = calculate_temporal_strength_with_diff(type1, type2, events_df)\n",
        "    temporal_strengths[i] = strength\n",
        "    temporal_details.append({\n",
        "        \"pair\": f\"{type1} ↔ {type2}\",\n",
        "        \"type1\": type1,\n",
        "        \"type2\": type2,\n",
        "        \"strength\": strength,\n",
        "        **details\n",
        "    })\n",
        "\n",
        "    m = details['shared_events']\n",
        "    n = details['total_events']\n",
        "    total_time = details['total_time']\n",
        "    shared_contrib = details['shared_time_contrib']\n",
        "\n",
        "    print(f\"{type1} ↔ {type2}: {strength:.3f} (shared:{m}/{n}, \"\n",
        "          f\"total_time:{total_time:.0f}s, shared_contrib:{shared_contrib:.0f}s)\")\n",
        "\n",
        "    if (i + 1) % 25 == 0:\n",
        "        elapsed = time.time() - start_time\n",
        "        rate = (i + 1) / elapsed\n",
        "        eta = (len(type_pairs) - i - 1) / rate if rate > 0 else 0\n",
        "        print(f\"  Progress: {i+1}/{len(type_pairs)} ({(i+1)/len(type_pairs)*100:.1f}%) - \"\n",
        "              f\"Rate: {rate:.1f}/sec - ETA: {eta:.1f}s\")\n",
        "\n",
        "elapsed_total = time.time() - start_time\n",
        "print(f\"\\nCompleted in {elapsed_total:.2f} seconds!\")\n",
        "print(f\"Average time per pair: {elapsed_total/len(type_pairs)*1000:.1f}ms\")"
      ],
      "metadata": {
        "id": "tsNNal60NpEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3f: Combined 4-Factor Strength Calculation\n",
        "weights = {\"o2o\": 0.4, \"activity\": 0.3, \"cooccurrence\": 0.2, \"temporal\": 0.1}\n",
        "combined_strengths = np.zeros(n_pairs)\n",
        "combined_details = []\n",
        "\n",
        "print(\"Combined 4-Factor Strengths:\")\n",
        "for i, (type1, type2) in enumerate(type_pairs):\n",
        "    o2o_str = o2o_strengths[i]\n",
        "    act_str = activity_strengths[i]\n",
        "    coo_str = cooccurrence_strengths[i]\n",
        "    tmp_str = temporal_strengths[i]\n",
        "\n",
        "    combined_strength = (\n",
        "        weights[\"o2o\"] * o2o_str +\n",
        "        weights[\"activity\"] * act_str +\n",
        "        weights[\"cooccurrence\"] * coo_str +\n",
        "        weights[\"temporal\"] * tmp_str\n",
        "    )\n",
        "\n",
        "    combined_strength = min(combined_strength, 1.0)\n",
        "    combined_strengths[i] = combined_strength\n",
        "\n",
        "    combined_details.append({\n",
        "        \"pair\": f\"{type1} ↔ {type2}\",\n",
        "        \"type1\": type1,\n",
        "        \"type2\": type2,\n",
        "        \"combined_strength\": combined_strength,\n",
        "        \"o2o_strength\": o2o_str,\n",
        "        \"activity_strength\": act_str,\n",
        "        \"cooccurrence_strength\": coo_str,\n",
        "        \"temporal_strength\": tmp_str\n",
        "    })\n",
        "\n",
        "    print(f\"{type1} ↔ {type2}: {combined_strength:.3f} \"\n",
        "          f\"[O2O:{o2o_str:.3f} Act:{act_str:.3f} Coo:{coo_str:.3f} Tmp:{tmp_str:.3f}]\")"
      ],
      "metadata": {
        "id": "rNap5iOmNtSC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding edges to graph and assigning strength to edges\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from itertools import combinations\n",
        "import time\n",
        "\n",
        "def create_relationship_graph_and_cluster(object_types, combined_strengths):\n",
        "    G = nx.Graph()\n",
        "\n",
        "    total_pairs = len(list(combinations(object_types, 2)))\n",
        "    print(f\"Processing {total_pairs} type pairs with all 4 factors...\")\n",
        "\n",
        "    type_pairs = list(combinations(object_types, 2))\n",
        "    all_strengths = []\n",
        "    pair_strengths = {}\n",
        "\n",
        "    print(\"\\nUsing provided combined strengths array...\")\n",
        "    for i, (type1, type2) in enumerate(type_pairs):\n",
        "        print(f\"Processing pair {i+1}/{total_pairs}: {type1} ↔ {type2}\")\n",
        "\n",
        "        strength = combined_strengths[i]\n",
        "        pair_strengths[(type1, type2)] = strength\n",
        "        if strength > 0:\n",
        "            all_strengths.append(strength)\n",
        "\n",
        "        print(f\"  → Strength: {strength:.3f}\")\n",
        "\n",
        "    if len(all_strengths) == 0:\n",
        "        print(\"\\nNo relationships found!\")\n",
        "        return [], G\n",
        "\n",
        "    all_strengths = np.array(all_strengths)\n",
        "    print(f\"\\nStrength Distribution:\")\n",
        "    print(f\"  Non-zero relationships: {len(all_strengths)}\")\n",
        "    print(f\"  Min: {all_strengths.min():.3f}\")\n",
        "    print(f\"  Max: {all_strengths.max():.3f}\")\n",
        "    print(f\"  Mean: {all_strengths.mean():.3f}\")\n",
        "    print(f\"  Std: {all_strengths.std():.3f}\")\n",
        "\n",
        "    strength_range = all_strengths.max() - all_strengths.min()\n",
        "    coefficient_of_variation = all_strengths.std() / all_strengths.mean() if all_strengths.mean() > 0 else 0\n",
        "\n",
        "    print(f\"  Range: {strength_range:.3f}\")\n",
        "    print(f\"  Coefficient of Variation: {coefficient_of_variation:.3f}\")\n",
        "\n",
        "    if strength_range < 0.1 or coefficient_of_variation < 0.2:\n",
        "        if all_strengths.mean() > 0.6:\n",
        "            strong_threshold = all_strengths.min() - 0.001\n",
        "            medium_threshold = -1\n",
        "            category = \"strong\"\n",
        "        elif all_strengths.mean() > 0.3:\n",
        "            strong_threshold = 2\n",
        "            medium_threshold = all_strengths.min() - 0.001\n",
        "            category = \"medium\"\n",
        "        else:\n",
        "            strong_threshold = 2\n",
        "            medium_threshold = 2\n",
        "            category = \"weak\"\n",
        "\n",
        "        print(f\"\\nEdge Case Detected: All strengths are similar (range={strength_range:.3f})\")\n",
        "        print(f\"   Assigning all relationships as '{category}'\")\n",
        "\n",
        "    else:\n",
        "        strong_threshold = np.percentile(all_strengths, 75)\n",
        "        medium_threshold = np.percentile(all_strengths, 25)\n",
        "\n",
        "        print(f\"\\nAdaptive Thresholds (Percentile-based):\")\n",
        "        print(f\"  Strong (top 25%): ≥ {strong_threshold:.3f}\")\n",
        "        print(f\"  Medium (middle 50%): {medium_threshold:.3f} to {strong_threshold:.3f}\")\n",
        "        print(f\"  Weak (bottom 25%): < {medium_threshold:.3f}\")\n",
        "\n",
        "    print(f\"\\nBuilding graph with adaptive thresholds...\")\n",
        "\n",
        "    edge_counts = {\"strong\": 0, \"medium\": 0, \"weak\": 0}\n",
        "\n",
        "    for (type1, type2), strength in pair_strengths.items():\n",
        "        if strength > 0:\n",
        "            if strength >= strong_threshold:\n",
        "                edge_type = \"strong\"\n",
        "            elif strength >= medium_threshold:\n",
        "                edge_type = \"medium\"\n",
        "            else:\n",
        "                edge_type = \"weak\"\n",
        "\n",
        "            G.add_edge(type1, type2, weight=strength, type=edge_type)\n",
        "            edge_counts[edge_type] += 1\n",
        "            print(f\"  {type1} ↔ {type2}: {strength:.3f} ({edge_type})\")\n",
        "\n",
        "    print(f\"\\nGraph Construction Summary:\")\n",
        "    print(f\"  Strong edges: {edge_counts['strong']}\")\n",
        "    print(f\"  Medium edges: {edge_counts['medium']}\")\n",
        "    print(f\"  Weak edges: {edge_counts['weak']}\")\n",
        "    print(f\"  Total edges: {sum(edge_counts.values())}\")\n",
        "\n",
        "    clusters = list(nx.connected_components(G))\n",
        "    print(f\"\\nClustering complete! Found {len(clusters)} clusters\")\n",
        "\n",
        "    return clusters, G\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "print(\"Starting adaptive threshold relationship analysis...\")\n",
        "clusters, relationship_graph = create_relationship_graph_and_cluster(object_types, combined_strengths)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Total processing time: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "NUI28V7OftVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting heat map and Graph\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def visualize_clusters_separately(clusters, relationship_graph, object_types):\n",
        "    \"\"\"Create separate heatmap and network graph visualizations\"\"\"\n",
        "\n",
        "    # Create similarity matrix from the graph\n",
        "    n_types = len(object_types)\n",
        "    similarity_matrix = np.zeros((n_types, n_types))\n",
        "    np.fill_diagonal(similarity_matrix, 1.0)\n",
        "\n",
        "    # Fill matrix from graph edges\n",
        "    for u, v, data in relationship_graph.edges(data=True):\n",
        "        i = object_types.index(u)\n",
        "        j = object_types.index(v)\n",
        "        weight = data['weight']\n",
        "        similarity_matrix[i][j] = weight\n",
        "        similarity_matrix[j][i] = weight\n",
        "\n",
        "    # Create similarity matrix heatmap\n",
        "    plt.figure(figsize=(9, 9))\n",
        "    sns.heatmap(similarity_matrix,\n",
        "                xticklabels=object_types,\n",
        "                yticklabels=object_types,\n",
        "                annot=True,\n",
        "                fmt='.3f',\n",
        "                cmap='RdYlBu_r',\n",
        "                center=0.5,\n",
        "                square=True,\n",
        "                linewidths=0.5,\n",
        "                cbar_kws={'label': 'Similarity Strength'})\n",
        "\n",
        "    plt.title('Object Type Similarity Matrix', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Object Types', fontsize=12)\n",
        "    plt.ylabel('Object Types', fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Create network graph\n",
        "    plt.figure(figsize=(9, 9))\n",
        "\n",
        "    pos = nx.spring_layout(relationship_graph, k=3, iterations=50)\n",
        "\n",
        "    # Draw nodes\n",
        "    nx.draw_networkx_nodes(relationship_graph, pos,\n",
        "                          node_color='lightblue',\n",
        "                          node_size=3000,\n",
        "                          alpha=0.7)\n",
        "\n",
        "    # Draw labels\n",
        "    nx.draw_networkx_labels(relationship_graph, pos,\n",
        "                           font_size=10,\n",
        "                           font_weight='bold')\n",
        "\n",
        "    # Draw edges by type\n",
        "    strong_edges = [(u, v) for u, v, d in relationship_graph.edges(data=True) if d['type'] == 'strong']\n",
        "    medium_edges = [(u, v) for u, v, d in relationship_graph.edges(data=True) if d['type'] == 'medium']\n",
        "    weak_edges = [(u, v) for u, v, d in relationship_graph.edges(data=True) if d['type'] == 'weak']\n",
        "\n",
        "    if strong_edges:\n",
        "        nx.draw_networkx_edges(relationship_graph, pos, edgelist=strong_edges,\n",
        "                              edge_color='red', width=3, alpha=0.8, label='Strong')\n",
        "    if medium_edges:\n",
        "        nx.draw_networkx_edges(relationship_graph, pos, edgelist=medium_edges,\n",
        "                              edge_color='orange', width=2, alpha=0.6, label='Medium')\n",
        "    if weak_edges:\n",
        "        nx.draw_networkx_edges(relationship_graph, pos, edgelist=weak_edges,\n",
        "                              edge_color='gray', width=1, alpha=0.4, label='Weak')\n",
        "\n",
        "    # Add edge labels\n",
        "    edge_labels = {(u, v): f\"{d['weight']:.3f}\" for u, v, d in relationship_graph.edges(data=True)}\n",
        "    nx.draw_networkx_edge_labels(relationship_graph, pos, edge_labels, font_size=8)\n",
        "\n",
        "    plt.title('Object Type Relationship Network', fontsize=16, fontweight='bold')\n",
        "    plt.legend()\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print cluster summary\n",
        "    print(f\"\\nCluster Summary:\")\n",
        "    for i, cluster in enumerate(clusters, 1):\n",
        "        print(f\"Cluster {i}: {list(cluster)} ({len(cluster)} nodes)\")\n",
        "\n",
        "    # Print network statistics\n",
        "    total_edges = relationship_graph.number_of_edges()\n",
        "    edge_types = {'strong': len(strong_edges), 'medium': len(medium_edges), 'weak': len(weak_edges)}\n",
        "\n",
        "    print(f\"\\nNetwork Statistics:\")\n",
        "    for edge_type, count in edge_types.items():\n",
        "        percentage = (count / total_edges * 100) if total_edges > 0 else 0\n",
        "        print(f\"  {edge_type.capitalize()} edges: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    print(f\"  Total edges: {total_edges}\")\n",
        "    print(f\"  Total clusters: {len(clusters)}\")\n",
        "    print(f\"  Total object types: {len(object_types)}\")\n",
        "\n",
        "visualize_clusters_separately(clusters, relationship_graph, object_types)"
      ],
      "metadata": {
        "id": "7ete43a7jJSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cluster Analysis\n",
        "def analyze_cluster_strong_relationships(clusters, relationship_graph):\n",
        "    \"\"\"Analyze and print strong relationships for each object in every cluster\"\"\"\n",
        "\n",
        "    print(f\"\\n{'-'*70}\")\n",
        "    print(\"Clustering Analysis\")\n",
        "    print(f\"{'-'*70}\")\n",
        "\n",
        "    for cluster_idx, cluster in enumerate(clusters, 1):\n",
        "        cluster_nodes = list(cluster)\n",
        "\n",
        "        print(f\"\\n{'-'*50}\")\n",
        "        print(f\"CLUSTER {cluster_idx}\")\n",
        "        print(f\"{'-'*50}\")\n",
        "        print(f\"Cluster Members: {cluster_nodes}\")\n",
        "        print(f\"Total Objects: {len(cluster_nodes)}\")\n",
        "\n",
        "        # For each object in the cluster, find its strong relationships\n",
        "        for obj in cluster_nodes:\n",
        "            print(f\"\\n{'-'*40}\")\n",
        "            print(f\"OBJECT: {obj}\")\n",
        "            print(f\"{'-'*40}\")\n",
        "\n",
        "            # Get all edges connected to this object within the cluster\n",
        "            connected_edges = []\n",
        "            for u, v, data in relationship_graph.edges(data=True):\n",
        "                if (u == obj and v in cluster_nodes) or (v == obj and u in cluster_nodes):\n",
        "                    other_node = v if u == obj else u\n",
        "                    if other_node != obj:  # Avoid self-loops\n",
        "                        connected_edges.append((other_node, data['weight'], data['type']))\n",
        "\n",
        "            # Filter for strong relationships\n",
        "            strong_relationships = [\n",
        "                (other_node, weight, edge_type)\n",
        "                for other_node, weight, edge_type in connected_edges\n",
        "                if edge_type == 'strong'\n",
        "            ]\n",
        "\n",
        "            if strong_relationships:\n",
        "                print(f\"Strong Relationships ({len(strong_relationships)}):\")\n",
        "                # Sort by weight in descending order\n",
        "                strong_relationships.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "                for other_node, weight, edge_type in strong_relationships:\n",
        "                    print(f\"  → {other_node}: {weight:.3f}\")\n",
        "            else:\n",
        "                print(\"Strong Relationships: None\")\n",
        "\n",
        "            # Also show all relationships for context\n",
        "            if connected_edges:\n",
        "                print(f\"\\nAll Relationships within Cluster ({len(connected_edges)}):\")\n",
        "                # Sort all relationships by weight\n",
        "                all_sorted = sorted(connected_edges, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "                for other_node, weight, edge_type in all_sorted:\n",
        "                    edge_symbol = {\n",
        "                        'strong': '●',\n",
        "                        'medium': '◐',\n",
        "                        'weak': '○'\n",
        "                    }.get(edge_type, '?')\n",
        "\n",
        "                    print(f\"  {edge_symbol} {other_node}: {weight:.3f} ({edge_type})\")\n",
        "            else:\n",
        "                print(\"\\nAll Relationships: None (isolated node)\")\n",
        "\n",
        "    # Summary of strong relationships within clusters\n",
        "    print(f\"\\n{'-'*70}\")\n",
        "    print(\"STRONG RELATIONSHIPS\")\n",
        "    print(f\"{'-'*70}\")\n",
        "\n",
        "    total_strong_edges = len([(u, v) for u, v, data in relationship_graph.edges(data=True) if data['type'] == 'strong'])\n",
        "\n",
        "    print(f\"Total Strong Relationships: {total_strong_edges}\")\n",
        "\n",
        "    # Find objects with most strong relationships\n",
        "    strong_counts = {}\n",
        "    for u, v, data in relationship_graph.edges(data=True):\n",
        "        if data['type'] == 'strong':\n",
        "            strong_counts[u] = strong_counts.get(u, 0) + 1\n",
        "            strong_counts[v] = strong_counts.get(v, 0) + 1\n",
        "\n",
        "    if strong_counts:\n",
        "        print(f\"\\nObjects with Most Strong Relationships:\")\n",
        "        sorted_objects = sorted(strong_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "        for obj, count in sorted_objects[:5]:  # Top 5\n",
        "            cluster_num = next(i+1 for i, cluster in enumerate(clusters) if obj in cluster)\n",
        "            print(f\"  {obj}: {count} strong relationships (Cluster {cluster_num})\")\n",
        "\n",
        "    # Cluster-level statistics\n",
        "    print(f\"\\nCluster Statistics:\")\n",
        "    for cluster_idx, cluster in enumerate(clusters, 1):\n",
        "        cluster_nodes = list(cluster)\n",
        "        cluster_subgraph = relationship_graph.subgraph(cluster_nodes)\n",
        "\n",
        "        strong_edges_in_cluster = len([(u, v) for u, v, data in cluster_subgraph.edges(data=True) if data['type'] == 'strong'])\n",
        "        total_edges_in_cluster = cluster_subgraph.number_of_edges()\n",
        "\n",
        "        print(f\"  Cluster {cluster_idx}: {len(cluster_nodes)} nodes, {total_edges_in_cluster} edges, {strong_edges_in_cluster} strong edges\")\n",
        "\n",
        "    print(f\"\\n{'-'*70}\")\n",
        "\n",
        "analyze_cluster_strong_relationships(clusters, relationship_graph)"
      ],
      "metadata": {
        "id": "AJeJqh5vt--_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}